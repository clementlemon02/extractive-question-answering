{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clementlemon02/extractive-question-answering/blob/main/RoBERTa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a5e1ed5-62cc-4639-bce0-d58cfb002f49",
      "metadata": {
        "id": "6a5e1ed5-62cc-4639-bce0-d58cfb002f49",
        "outputId": "b7125d0d-f2fe-4b65-d1fe-5920e62fa5cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset version: 1.1\n",
            "Number of articles: 442\n",
            "\n",
            "Sample article title: University_of_Notre_Dame\n",
            "Number of paragraphs in the first article: 55\n",
            "\n",
            "First paragraph context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputed\n",
            "Number of Q&A pairs in this paragraph: 5\n",
            "\n",
            "Sample question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "Sample answer: Saint Bernadette Soubirous\n"
          ]
        }
      ],
      "source": [
        "squad_data = json.load(open('train-v1.1.json', 'rb'))\n",
        "\n",
        "# Dataset overview\n",
        "print(f\"Dataset version: {squad_data['version']}\")\n",
        "print(f\"Number of articles: {len(squad_data['data'])}\")\n",
        "\n",
        "# Sample entry: the first article, its title, and the first question-answer pair\n",
        "print(f\"\\nSample article title: {squad_data['data'][0]['title']}\")\n",
        "print(f\"Number of paragraphs in the first article: {len(squad_data['data'][0]['paragraphs'])}\")\n",
        "\n",
        "# Sample paragraph and QA pair\n",
        "first_paragraph = squad_data['data'][0]['paragraphs'][0]\n",
        "print(f\"\\nFirst paragraph context: {first_paragraph['context'][:500]}\")  # Displaying part of the context\n",
        "print(f\"Number of Q&A pairs in this paragraph: {len(first_paragraph['qas'])}\")\n",
        "\n",
        "# Sample Question-Answer pair\n",
        "sample_qa = first_paragraph['qas'][0]\n",
        "print(f\"\\nSample question: {sample_qa['question']}\")\n",
        "print(f\"Sample answer: {sample_qa['answers'][0]['text']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7afc7012-ca13-4c3d-91a7-5a35911b1dcc",
      "metadata": {
        "id": "7afc7012-ca13-4c3d-91a7-5a35911b1dcc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "class SQuADDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512, stride=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.stride = stride\n",
        "\n",
        "        self.dataset = []\n",
        "\n",
        "        for article in data['data']:\n",
        "            for paragraph in article['paragraphs']:\n",
        "                context = paragraph['context']\n",
        "                for qa in paragraph['qas']:\n",
        "                    question = qa['question']\n",
        "                    question_id = qa['id']  # Get the question ID\n",
        "\n",
        "                    for answer in qa['answers']:\n",
        "                        self.dataset.append({\n",
        "                            'question_id': question_id,\n",
        "                            'question': question,\n",
        "                            'context': context,\n",
        "                            'answer_start': answer['answer_start'],\n",
        "                            'answer_text': answer['text']\n",
        "                        })\n",
        "\n",
        "        print(f\"Dataset initialized with {len(self.dataset)} items\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "\n",
        "        question = item['question']\n",
        "        context = item['context']\n",
        "        answer_start = item['answer_start']\n",
        "        answer_text = item['answer_text']\n",
        "        question_id = item['question_id']\n",
        "\n",
        "        # Tokenize the question and context with a sliding window\n",
        "        tokenized_item = self.tokenizer(\n",
        "            question,\n",
        "            context,\n",
        "            max_length=self.max_length,\n",
        "            truncation='only_second',\n",
        "            stride=self.stride,\n",
        "            return_overflowing_tokens=True,\n",
        "            return_offsets_mapping=True,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Prepare start and end positions for the answer\n",
        "        offset_mapping = tokenized_item['offset_mapping'][0]\n",
        "        cls_token_idx = self.tokenizer.cls_token_id  # Typically, CLS token index is 0\n",
        "        start_positions = []\n",
        "        end_positions = []\n",
        "\n",
        "        for i, (offset) in enumerate(offset_mapping):\n",
        "            if offset[0] <= answer_start < offset[1]:\n",
        "                start_positions.append(i)\n",
        "            if offset[0] < answer_start + len(answer_text) <= offset[1]:\n",
        "                end_positions.append(i)\n",
        "\n",
        "        # If no answer is found, set start and end positions to the CLS token (0)\n",
        "        if not start_positions:\n",
        "            start_positions = [cls_token_idx]\n",
        "        if not end_positions:\n",
        "            end_positions = [cls_token_idx]\n",
        "\n",
        "        # Select the first (or only) valid span\n",
        "        start_position = start_positions[0]\n",
        "        end_position = end_positions[0]\n",
        "\n",
        "        return {\n",
        "            'input_ids': tokenized_item['input_ids'][0],\n",
        "            'attention_mask': tokenized_item['attention_mask'][0],\n",
        "            'start_positions': torch.tensor(start_position, dtype=torch.long),\n",
        "            'end_positions': torch.tensor(end_position, dtype=torch.long),\n",
        "            'question_id': question_id\n",
        "        }\n",
        "\n",
        "    def export_to_csv(self, filename):\n",
        "        # Prepare data for export\n",
        "        df_data = []\n",
        "        for item in self.dataset:\n",
        "            df_data.append({\n",
        "                'question_id': item['question_id'],\n",
        "                'question': item['question'],\n",
        "                'context': item['context'],\n",
        "                'answer_start': item['answer_start'],\n",
        "                'answer_text': item['answer_text']\n",
        "            })\n",
        "\n",
        "        # Create a DataFrame and save it to CSV\n",
        "        df = pd.DataFrame(df_data)\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"Dataset exported to {filename}\")\n",
        "\n",
        "    def export_to_squad_format(self, filename):\n",
        "        # Create a new data structure in SQuAD format\n",
        "        squad_format = {\n",
        "            \"version\": \"v2.0\",\n",
        "            \"data\": []\n",
        "        }\n",
        "\n",
        "        # Group items by context\n",
        "        context_groups = {}\n",
        "        for item in self.dataset:\n",
        "            if item['context'] not in context_groups:\n",
        "                context_groups[item['context']] = []\n",
        "            context_groups[item['context']].append(item)\n",
        "\n",
        "        # Build the SQuAD format structure\n",
        "        for context, items in context_groups.items():\n",
        "            article = {\n",
        "                \"title\": \"Generated Article\",\n",
        "                \"paragraphs\": [{\n",
        "                    \"context\": context,\n",
        "                    \"qas\": []\n",
        "                }]\n",
        "            }\n",
        "\n",
        "            for item in items:\n",
        "                qa = {\n",
        "                    \"question\": item['question'],\n",
        "                    \"id\": item['question_id'],\n",
        "                    \"answers\": [{\n",
        "                        \"text\": item['answer_text'],\n",
        "                        \"answer_start\": item['answer_start']\n",
        "                    }]\n",
        "                }\n",
        "                article['paragraphs'][0]['qas'].append(qa)\n",
        "\n",
        "            squad_format['data'].append(article)\n",
        "\n",
        "        # Write to JSON file\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(squad_format, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"Dataset exported in SQuAD format to {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3444732a-f6e0-4ac3-81cb-2c24be310c03",
      "metadata": {
        "id": "3444732a-f6e0-4ac3-81cb-2c24be310c03",
        "outputId": "109cf5af-c0fe-455c-c48e-2141e01dfff4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded: RobertaForQuestionAnswering\n",
            "Model name: deepset/roberta-base-squad2\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "model_name = 'deepset/roberta-base-squad2'  # RoBERTa model fine-tuned on SQuAD 2.0\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "# Verify the model loaded correctly\n",
        "print(f\"Model loaded: {model.__class__.__name__}\")\n",
        "print(f\"Model name: {model.name_or_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "303ce9e5-395d-4f7a-a907-b485b407bdb8",
      "metadata": {
        "id": "303ce9e5-395d-4f7a-a907-b485b407bdb8",
        "outputId": "544be3d0-2334-41de-a583-5d6ff382b542",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset initialized with 87599 items\n",
            "\n",
            "Testing final dataset access:\n",
            "\n",
            "Sample 0:\n",
            "  input_ids: torch.Size([512])\n",
            "  attention_mask: torch.Size([512])\n",
            "  start_positions: torch.Size([])\n",
            "  end_positions: torch.Size([])\n",
            "  question_id: 5733be284776f41900661182\n",
            "  Start position: 135\n",
            "  End position: 142\n",
            "  Decoded answer:  Saint Bernadette Soubirous\n",
            "  Full input: <s>To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?</s></s>Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Vir...\n",
            "\n",
            "Sample 1:\n",
            "  input_ids: torch.Size([512])\n",
            "  attention_mask: torch.Size([512])\n",
            "  start_positions: torch.Size([])\n",
            "  end_positions: torch.Size([])\n",
            "  question_id: 5733be284776f4190066117f\n",
            "  Start position: 54\n",
            "  End position: 58\n",
            "  Decoded answer:  a copper statue of Christ\n",
            "  Full input: <s>What is in front of the Notre Dame Main Building?</s></s>Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately ...\n",
            "\n",
            "Sample 2:\n",
            "  input_ids: torch.Size([512])\n",
            "  attention_mask: torch.Size([512])\n",
            "  start_positions: torch.Size([])\n",
            "  end_positions: torch.Size([])\n",
            "  question_id: 5733be284776f41900661180\n",
            "  Start position: 81\n",
            "  End position: 83\n",
            "  Decoded answer:  the Main Building\n",
            "  Full input: <s>The Basilica of the Sacred heart at Notre Dame is beside to which structure?</s></s>Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of th...\n",
            "\n",
            "Verifying start and end positions:\n",
            "Sample 0:\n",
            "  Input length: 512\n",
            "  Start position: 135\n",
            "  End position: 142\n",
            "Sample 1:\n",
            "  Input length: 512\n",
            "  Start position: 54\n",
            "  End position: 58\n",
            "Sample 2:\n",
            "  Input length: 512\n",
            "  Start position: 81\n",
            "  End position: 83\n"
          ]
        }
      ],
      "source": [
        "# Recreate the dataset with the updated class\n",
        "dataset = SQuADDataset(squad_data, tokenizer)\n",
        "print(\"\\nTesting final dataset access:\")\n",
        "for i in range(3):  # Print info for first 3 items\n",
        "    sample = dataset[i]\n",
        "    print(f\"\\nSample {i}:\")\n",
        "    for k, v in sample.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            print(f\"  {k}: {v.shape}\")\n",
        "        else:\n",
        "            print(f\"  {k}: {v}\")\n",
        "\n",
        "    # Print start and end positions\n",
        "    start_pos = sample['start_positions'].item()\n",
        "    end_pos = sample['end_positions'].item()\n",
        "    print(f\"  Start position: {start_pos}\")\n",
        "    print(f\"  End position: {end_pos}\")\n",
        "\n",
        "    # Decode the answer span\n",
        "    answer_span = sample['input_ids'][start_pos:end_pos+1]\n",
        "    decoded_answer = tokenizer.decode(answer_span)\n",
        "    print(f\"  Decoded answer: {decoded_answer}\")\n",
        "\n",
        "    # Decode the full input\n",
        "    full_input = tokenizer.decode(sample['input_ids'])\n",
        "    print(f\"  Full input: {full_input[:200]}...\")  # Print first 200 characters\n",
        "\n",
        "# Verify that start and end positions are within the expected range\n",
        "print(\"\\nVerifying start and end positions:\")\n",
        "for i in range(3):\n",
        "    sample = dataset[i]\n",
        "    start_pos = sample['start_positions'].item()\n",
        "    end_pos = sample['end_positions'].item()\n",
        "    input_length = sample['input_ids'].shape[0]\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(f\"  Input length: {input_length}\")\n",
        "    print(f\"  Start position: {start_pos}\")\n",
        "    print(f\"  End position: {end_pos}\")\n",
        "    assert 0 <= start_pos < input_length, f\"Start position {start_pos} out of range\"\n",
        "    assert 0 <= end_pos < input_length, f\"End position {end_pos} out of range\"\n",
        "    assert start_pos <= end_pos, f\"Start position {start_pos} greater than end position {end_pos}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42a9112f-9b3c-4f3e-821e-16230f4bc8cb",
      "metadata": {
        "id": "42a9112f-9b3c-4f3e-821e-16230f4bc8cb",
        "outputId": "9118c7b7-d911-4e17-824e-5a2e3f25ecc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset initialized with 87599 items\n",
            "Dataset initialized with 34726 items\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Load the dataset from local file\n",
        "file_path = 'train-v1.1.json'\n",
        "\n",
        "# Open and load the JSON file\n",
        "with open(file_path, 'r') as file:\n",
        "    squad_data = json.load(file)\n",
        "\n",
        "with open('dev-v1.1.json', 'r', encoding='utf-8') as f:\n",
        "    val_squad_data = json.load(f)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "train_dataset = SQuADDataset(squad_data, tokenizer, max_length=512, stride=128)\n",
        "val_dataset = SQuADDataset(val_squad_data, tokenizer, max_length=512, stride=128)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84cc3289-d37d-4d2d-b461-17a24069ceac",
      "metadata": {
        "id": "84cc3289-d37d-4d2d-b461-17a24069ceac",
        "outputId": "4fc47ce8-18d8-44d3-8153-9a26d541b333",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch shapes:\n",
            "  input_ids: torch.Size([16, 512])\n",
            "  attention_mask: torch.Size([16, 512])\n",
            "  start_positions: torch.Size([16])\n",
            "  end_positions: torch.Size([16])\n",
            "  question_id: <class 'list'> (length: 16)\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Assuming you've already created your dataset\n",
        "batch_size = 16  # You can adjust this based on your GPU memory\n",
        "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Test the dataloader\n",
        "# Test the dataloader\n",
        "for batch in dataloader:\n",
        "    print(\"Batch shapes:\")\n",
        "    for k, v in batch.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            print(f\"  {k}: {v.shape}\")\n",
        "        else:\n",
        "            print(f\"  {k}: {type(v)} (length: {len(v)})\")\n",
        "    break  # Just print the first batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35391220-8fcc-4cfe-9990-d33377653b03",
      "metadata": {
        "id": "35391220-8fcc-4cfe-9990-d33377653b03"
      },
      "outputs": [],
      "source": [
        "from transformers import default_data_collator\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForQuestionAnswering, AdamW, get_scheduler\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def train_model(\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    model_name=\"bert-base-uncased\",\n",
        "    batch_size=8,\n",
        "    epochs=3,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=0,\n",
        "    output_dir=\"model_output\"\n",
        "):\n",
        "    # Load the tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "    # Set up the DataLoader for training and validation datasets\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=default_data_collator\n",
        "    )\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=default_data_collator\n",
        "    )\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Set up the optimizer and learning rate scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    num_training_steps = epochs * len(train_dataloader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        name=\"linear\", optimizer=optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    # Set up a progress bar\n",
        "    progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            progress_bar.update(1)\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "        print(f\"Average training loss: {avg_train_loss}\")\n",
        "\n",
        "        # Evaluate on the validation set\n",
        "        eval_loss = evaluate(model, val_dataloader, device)\n",
        "        print(f\"Validation loss: {eval_loss}\")\n",
        "\n",
        "        # Save the model at the end of each epoch\n",
        "        model.save_pretrained(f\"{output_dir}/checkpoint_epoch_{epoch+1}\")\n",
        "        tokenizer.save_pretrained(f\"{output_dir}/checkpoint_epoch_{epoch+1}\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            total_loss += outputs.loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(val_dataloader)\n",
        "    model.train()  # Set back to training mode\n",
        "    return avg_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "762e6a84-93fc-40e3-ae2a-9b974cac7f10",
      "metadata": {
        "id": "762e6a84-93fc-40e3-ae2a-9b974cac7f10"
      },
      "outputs": [],
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "def custom_collator(features):\n",
        "    # Extract question_ids from the dataset\n",
        "    question_ids = [feature['question_id'] for feature in features]\n",
        "\n",
        "    # Use the default data collator to handle the rest of the tensors\n",
        "    batch = default_data_collator(features)\n",
        "\n",
        "    # Add question_ids back to the batch\n",
        "    batch['question_id'] = question_ids\n",
        "\n",
        "    return batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16c0a275-b11e-4b77-b77e-c44f2bf10ffb",
      "metadata": {
        "id": "16c0a275-b11e-4b77-b77e-c44f2bf10ffb"
      },
      "outputs": [],
      "source": [
        "def predict(model, val_dataset, tokenizer, batch_size=8, device=None):\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Set device (use GPU if available)\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Set up DataLoader for validation dataset, using the custom collator\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collator)\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # Create a progress bar based on the number of batches\n",
        "    progress_bar = tqdm(val_dataloader, desc=\"Predicting\", leave=True)\n",
        "\n",
        "    # No gradients needed during evaluation\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            # Move tensors to device, ignore `question_id`\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'question_id'}\n",
        "\n",
        "            # Get model outputs\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            # Get start and end logits\n",
        "            start_logits = outputs.start_logits\n",
        "            end_logits = outputs.end_logits\n",
        "\n",
        "            # Iterate over each example in the batch\n",
        "            for i in range(start_logits.size(0)):\n",
        "                start_idx = torch.argmax(start_logits[i]).item()\n",
        "                end_idx = torch.argmax(end_logits[i]).item()\n",
        "\n",
        "                # Get input IDs (tokens) and convert to words\n",
        "                input_ids = batch['input_ids'][i]\n",
        "                tokens = tokenizer.convert_ids_to_tokens(input_ids[start_idx:end_idx+1])\n",
        "\n",
        "                # Convert tokens back to string\n",
        "                answer = tokenizer.convert_tokens_to_string(tokens)\n",
        "\n",
        "                # Retrieve the question ID from the batch (now included in the batch via the collator)\n",
        "                question_id = batch['question_id'][i]\n",
        "\n",
        "                # Save the question ID and predicted answer\n",
        "                predictions.append({\"question_id\": question_id, \"answer\": answer})\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce617432-4b06-40e8-ab81-ed0d1bd40d15",
      "metadata": {
        "id": "ce617432-4b06-40e8-ab81-ed0d1bd40d15",
        "outputId": "d9157d57-9159-49b6-8b57-2de80a7f1e77",
        "colab": {
          "referenced_widgets": [
            "c16c6af10eae4352a6b5c000d79a697d",
            "7bbff0dc797b40b691771da7b07f0b2c",
            "b7da3ca566844f49ad452ad3d95dde54",
            "c9223c9c63cd4117b978a3b29a10a83c",
            "6727f16709cc48c2881958c189a75b1a",
            "c05e45365d524205ab68b3ffc545becc",
            "673909fcc0f1436ebfe6cfde1ef65960",
            "24fe30430a2948ab83f0aecf58809da0",
            "6255c90ea83e4e64892eae8db8e43fcb",
            "33be58c971d04bd798790a94b7453ca9",
            "14f4aed4ebd0444e936f57c94a4096be"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/32850 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c16c6af10eae4352a6b5c000d79a697d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        }
      ],
      "source": [
        "train_model(\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    model_name=model_name,\n",
        "    batch_size=8,  # You can adjust batch size depending on your hardware\n",
        "    epochs=3,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=0,\n",
        "    output_dir=\"model_output\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a99a7862-1fde-4586-96ea-ebb61353e2d9",
      "metadata": {
        "id": "a99a7862-1fde-4586-96ea-ebb61353e2d9"
      },
      "outputs": [],
      "source": [
        "def save_predictions(predictions, filename=\"predictions.json\"):\n",
        "    # Convert the predictions into the required format: {question_id: answer}\n",
        "    formatted_predictions = {pred['question_id']: pred['answer'] for pred in predictions}\n",
        "\n",
        "    # Save the formatted predictions as a JSON file\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(formatted_predictions, f, indent=2)\n",
        "\n",
        "    print(f\"Predictions saved to {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "218cdc46-041a-4295-9773-58b9ac433ea9",
      "metadata": {
        "id": "218cdc46-041a-4295-9773-58b9ac433ea9"
      },
      "outputs": [],
      "source": [
        "# Run the prediction process\n",
        "predictions = predict(model, val_dataset, tokenizer)\n",
        "\n",
        "# Save the predictions in the required format\n",
        "save_predictions(predictions, \"predictions.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e5b9721-3aa8-475a-aced-015efacffcc9",
      "metadata": {
        "id": "1e5b9721-3aa8-475a-aced-015efacffcc9"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "# Run the evaluation script\n",
        "result = subprocess.run(['python', 'evaluate-v2.0.py', 'dev-v1.1.json', 'predictions.json'], capture_output=True, text=True)\n",
        "\n",
        "# Print the output\n",
        "print(result.stdout)\n",
        "\n",
        "# If there were any errors, print them\n",
        "if result.stderr:\n",
        "    print(\"Errors:\")\n",
        "    print(result.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "873c0fde-e168-470f-a153-37dc00761c32",
      "metadata": {
        "id": "873c0fde-e168-470f-a153-37dc00761c32"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PyTorch CUDA",
      "language": "python",
      "name": "torch_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c16c6af10eae4352a6b5c000d79a697d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7bbff0dc797b40b691771da7b07f0b2c",
              "IPY_MODEL_b7da3ca566844f49ad452ad3d95dde54",
              "IPY_MODEL_c9223c9c63cd4117b978a3b29a10a83c"
            ],
            "layout": "IPY_MODEL_6727f16709cc48c2881958c189a75b1a"
          }
        },
        "7bbff0dc797b40b691771da7b07f0b2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c05e45365d524205ab68b3ffc545becc",
            "placeholder": "​",
            "style": "IPY_MODEL_673909fcc0f1436ebfe6cfde1ef65960",
            "value": "  0%"
          }
        },
        "b7da3ca566844f49ad452ad3d95dde54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24fe30430a2948ab83f0aecf58809da0",
            "max": 32850,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6255c90ea83e4e64892eae8db8e43fcb",
            "value": 0
          }
        },
        "c9223c9c63cd4117b978a3b29a10a83c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33be58c971d04bd798790a94b7453ca9",
            "placeholder": "​",
            "style": "IPY_MODEL_14f4aed4ebd0444e936f57c94a4096be",
            "value": " 0/32850 [00:00&lt;?, ?it/s]"
          }
        },
        "6727f16709cc48c2881958c189a75b1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c05e45365d524205ab68b3ffc545becc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "673909fcc0f1436ebfe6cfde1ef65960": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24fe30430a2948ab83f0aecf58809da0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6255c90ea83e4e64892eae8db8e43fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33be58c971d04bd798790a94b7453ca9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14f4aed4ebd0444e936f57c94a4096be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}